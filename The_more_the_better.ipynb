{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "codemirror_mode": "r",
      "file_extension": ".r",
      "mimetype": "text/x-r-source",
      "name": "R",
      "pygments_lexer": "r",
      "version": "3.4.1"
    },
    "colab": {
      "name": "The_more_the_better.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jlokkerbol/masterclass/blob/main/The_more_the_better.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCbzhy8vaGhh",
        "outputId": "32da599c-c2b8-457a-b4d1-547e409ccba9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "install.packages('glmnet')\n",
        "install.packages('data.table')\n",
        "install.packages('caret')\n",
        "\n",
        "library(glmnet)\n",
        "library(data.table)\n",
        "library(caret)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependency ‘shape’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Loading required package: Matrix\n",
            "\n",
            "Loaded glmnet 4.0-2\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0spI0amZWXi"
      },
      "source": [
        "#The more the better?\n",
        "\n",
        "One of the ideas that people tend to have about machine learning, is that it is always better to have more data. This notebook considers how adding irrelevant data to your dataset impacts the performance of your model, i.e. the extent to which machine learning is able to distinguish between relevant and irrelevant information.\n",
        "\n",
        "The following steps are taken:\n",
        "- step 1: define datagenerating process and simulate data\n",
        "- step 2: split the data into training and test set\n",
        "- step 3: train model using all data\n",
        "- step 4: evaluate performance of full model\n",
        "- step 5: train model using only the relevant data\n",
        "- step 6: compare the performance of both models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z46naM6qbxDu",
        "outputId": "58759b8f-f82c-4142-b519-c5d5977f64ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# step 1: define datagenerating process and simulate data\n",
        "set.seed(12345)\n",
        "X1 <- rnorm(n = 180, mean = 20, sd = 5)\n",
        "X2 <- rnorm(n = 180, mean = 0, sd = 5)\n",
        "error <- rnorm(n = 180, mean = 0, sd = 100)\n",
        "Y <- 10+40*X1+20*X2+error\n",
        "df <- cbind(Y, X1, X2)\n",
        "df <- as.data.frame(df)\n",
        "head(df)\n",
        "\n",
        "#create non-relevant variables and add to the dataframe\n",
        "for (i in 3:100) {\n",
        "        df <- cbind(df,rnorm(n=180, mean=0, sd=1))\n",
        "        colnames(df)[i+1] <- paste(\"X\",i,sep=\"\")\n",
        "}\n",
        "\n",
        "head(df)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Y         X1       X2        \n",
              "1  798.3495 22.92764 -4.5586971\n",
              "2  765.7054 23.54733 -0.2452235\n",
              "3  776.4606 19.45348 -2.0269374\n",
              "4  813.3765 17.73251  5.6519090\n",
              "5 1014.5100 23.02944  4.0773237\n",
              "6  519.0936 10.91022  0.3820876"
            ],
            "text/latex": "A data.frame: 6 × 3\n\\begin{tabular}{r|lll}\n  & Y & X1 & X2\\\\\n  & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t1 &  798.3495 & 22.92764 & -4.5586971\\\\\n\t2 &  765.7054 & 23.54733 & -0.2452235\\\\\n\t3 &  776.4606 & 19.45348 & -2.0269374\\\\\n\t4 &  813.3765 & 17.73251 &  5.6519090\\\\\n\t5 & 1014.5100 & 23.02944 &  4.0773237\\\\\n\t6 &  519.0936 & 10.91022 &  0.3820876\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 6 × 3\n\n| <!--/--> | Y &lt;dbl&gt; | X1 &lt;dbl&gt; | X2 &lt;dbl&gt; |\n|---|---|---|---|\n| 1 |  798.3495 | 22.92764 | -4.5586971 |\n| 2 |  765.7054 | 23.54733 | -0.2452235 |\n| 3 |  776.4606 | 19.45348 | -2.0269374 |\n| 4 |  813.3765 | 17.73251 |  5.6519090 |\n| 5 | 1014.5100 | 23.02944 |  4.0773237 |\n| 6 |  519.0936 | 10.91022 |  0.3820876 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.frame: 6 × 3</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>Y</th><th scope=col>X1</th><th scope=col>X2</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td> 798.3495</td><td>22.92764</td><td>-4.5586971</td></tr>\n",
              "\t<tr><th scope=row>2</th><td> 765.7054</td><td>23.54733</td><td>-0.2452235</td></tr>\n",
              "\t<tr><th scope=row>3</th><td> 776.4606</td><td>19.45348</td><td>-2.0269374</td></tr>\n",
              "\t<tr><th scope=row>4</th><td> 813.3765</td><td>17.73251</td><td> 5.6519090</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>1014.5100</td><td>23.02944</td><td> 4.0773237</td></tr>\n",
              "\t<tr><th scope=row>6</th><td> 519.0936</td><td>10.91022</td><td> 0.3820876</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  Y         X1       X2         X3          X4         X5          X6        \n",
              "1  798.3495 22.92764 -4.5586971  2.12746303 -0.2915456 -0.78486098 -0.8257089\n",
              "2  765.7054 23.54733 -0.2452235 -0.18770648  0.9726278 -2.56005244  0.4902205\n",
              "3  776.4606 19.45348 -2.0269374  0.09875784 -0.2397067  0.07280078 -0.9265319\n",
              "4  813.3765 17.73251  5.6519090  1.91037815 -0.9017177  0.75024358  1.6025499\n",
              "5 1014.5100 23.02944  4.0773237  1.62145572 -0.6311743 -0.12824888 -0.8608910\n",
              "6  519.0936 10.91022  0.3820876  2.09306799 -1.6259751 -0.48786673 -0.3655133\n",
              "  X7         X8         X9         ⋯ X91        X92        X93       \n",
              "1  0.2092830  0.7548681 -0.2624089 ⋯  0.2170089  0.9811519 -0.1497657\n",
              "2  0.5836894  0.2102209 -1.2662017 ⋯  0.5958067  1.3969812  0.5189769\n",
              "3 -0.6860822 -0.6535798 -0.9027523 ⋯  0.8350547 -0.2559876  0.8028454\n",
              "4 -1.0184904  0.2518509  0.1460915 ⋯ -0.7519070  1.2994639  0.5629971\n",
              "5 -1.1347534 -0.4156504  1.3384244 ⋯  0.8735983  0.7651778 -1.7634525\n",
              "6 -0.4582505 -0.1920236  0.6668734 ⋯  0.2263574 -0.4766437 -2.8152937\n",
              "  X94        X95         X96        X97        X98        X99       \n",
              "1  2.7536749 -0.07124450 -0.3998977  0.9764391 -0.3016068 -1.4666577\n",
              "2  0.8732472  0.83001130 -1.3213831 -0.9214097 -1.2180848 -0.9668004\n",
              "3 -0.5068545 -0.31125485 -0.4995762  0.5872777  0.4780813 -0.8942023\n",
              "4  1.4382039 -0.51459724 -0.6148111  1.5750744  0.0106562 -0.9325735\n",
              "5  0.4803649  0.31325164 -0.9264642 -0.8271999  2.6599423 -1.0749029\n",
              "6 -0.9024205  0.02830745  1.3253200  0.3081907  0.5242000 -0.4868474\n",
              "  X100       \n",
              "1 -0.28744876\n",
              "2  0.03717712\n",
              "3 -1.34888323\n",
              "4 -0.76406130\n",
              "5  0.44507673\n",
              "6  1.78749391"
            ],
            "text/latex": "A data.frame: 6 × 101\n\\begin{tabular}{r|lllllllllllllllllllll}\n  & Y & X1 & X2 & X3 & X4 & X5 & X6 & X7 & X8 & X9 & ⋯ & X91 & X92 & X93 & X94 & X95 & X96 & X97 & X98 & X99 & X100\\\\\n  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & ⋯ & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl>\\\\\n\\hline\n\t1 &  798.3495 & 22.92764 & -4.5586971 &  2.12746303 & -0.2915456 & -0.78486098 & -0.8257089 &  0.2092830 &  0.7548681 & -0.2624089 & ⋯ &  0.2170089 &  0.9811519 & -0.1497657 &  2.7536749 & -0.07124450 & -0.3998977 &  0.9764391 & -0.3016068 & -1.4666577 & -0.28744876\\\\\n\t2 &  765.7054 & 23.54733 & -0.2452235 & -0.18770648 &  0.9726278 & -2.56005244 &  0.4902205 &  0.5836894 &  0.2102209 & -1.2662017 & ⋯ &  0.5958067 &  1.3969812 &  0.5189769 &  0.8732472 &  0.83001130 & -1.3213831 & -0.9214097 & -1.2180848 & -0.9668004 &  0.03717712\\\\\n\t3 &  776.4606 & 19.45348 & -2.0269374 &  0.09875784 & -0.2397067 &  0.07280078 & -0.9265319 & -0.6860822 & -0.6535798 & -0.9027523 & ⋯ &  0.8350547 & -0.2559876 &  0.8028454 & -0.5068545 & -0.31125485 & -0.4995762 &  0.5872777 &  0.4780813 & -0.8942023 & -1.34888323\\\\\n\t4 &  813.3765 & 17.73251 &  5.6519090 &  1.91037815 & -0.9017177 &  0.75024358 &  1.6025499 & -1.0184904 &  0.2518509 &  0.1460915 & ⋯ & -0.7519070 &  1.2994639 &  0.5629971 &  1.4382039 & -0.51459724 & -0.6148111 &  1.5750744 &  0.0106562 & -0.9325735 & -0.76406130\\\\\n\t5 & 1014.5100 & 23.02944 &  4.0773237 &  1.62145572 & -0.6311743 & -0.12824888 & -0.8608910 & -1.1347534 & -0.4156504 &  1.3384244 & ⋯ &  0.8735983 &  0.7651778 & -1.7634525 &  0.4803649 &  0.31325164 & -0.9264642 & -0.8271999 &  2.6599423 & -1.0749029 &  0.44507673\\\\\n\t6 &  519.0936 & 10.91022 &  0.3820876 &  2.09306799 & -1.6259751 & -0.48786673 & -0.3655133 & -0.4582505 & -0.1920236 &  0.6668734 & ⋯ &  0.2263574 & -0.4766437 & -2.8152937 & -0.9024205 &  0.02830745 &  1.3253200 &  0.3081907 &  0.5242000 & -0.4868474 &  1.78749391\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 6 × 101\n\n| <!--/--> | Y &lt;dbl&gt; | X1 &lt;dbl&gt; | X2 &lt;dbl&gt; | X3 &lt;dbl&gt; | X4 &lt;dbl&gt; | X5 &lt;dbl&gt; | X6 &lt;dbl&gt; | X7 &lt;dbl&gt; | X8 &lt;dbl&gt; | X9 &lt;dbl&gt; | ⋯ ⋯ | X91 &lt;dbl&gt; | X92 &lt;dbl&gt; | X93 &lt;dbl&gt; | X94 &lt;dbl&gt; | X95 &lt;dbl&gt; | X96 &lt;dbl&gt; | X97 &lt;dbl&gt; | X98 &lt;dbl&gt; | X99 &lt;dbl&gt; | X100 &lt;dbl&gt; |\n|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n| 1 |  798.3495 | 22.92764 | -4.5586971 |  2.12746303 | -0.2915456 | -0.78486098 | -0.8257089 |  0.2092830 |  0.7548681 | -0.2624089 | ⋯ |  0.2170089 |  0.9811519 | -0.1497657 |  2.7536749 | -0.07124450 | -0.3998977 |  0.9764391 | -0.3016068 | -1.4666577 | -0.28744876 |\n| 2 |  765.7054 | 23.54733 | -0.2452235 | -0.18770648 |  0.9726278 | -2.56005244 |  0.4902205 |  0.5836894 |  0.2102209 | -1.2662017 | ⋯ |  0.5958067 |  1.3969812 |  0.5189769 |  0.8732472 |  0.83001130 | -1.3213831 | -0.9214097 | -1.2180848 | -0.9668004 |  0.03717712 |\n| 3 |  776.4606 | 19.45348 | -2.0269374 |  0.09875784 | -0.2397067 |  0.07280078 | -0.9265319 | -0.6860822 | -0.6535798 | -0.9027523 | ⋯ |  0.8350547 | -0.2559876 |  0.8028454 | -0.5068545 | -0.31125485 | -0.4995762 |  0.5872777 |  0.4780813 | -0.8942023 | -1.34888323 |\n| 4 |  813.3765 | 17.73251 |  5.6519090 |  1.91037815 | -0.9017177 |  0.75024358 |  1.6025499 | -1.0184904 |  0.2518509 |  0.1460915 | ⋯ | -0.7519070 |  1.2994639 |  0.5629971 |  1.4382039 | -0.51459724 | -0.6148111 |  1.5750744 |  0.0106562 | -0.9325735 | -0.76406130 |\n| 5 | 1014.5100 | 23.02944 |  4.0773237 |  1.62145572 | -0.6311743 | -0.12824888 | -0.8608910 | -1.1347534 | -0.4156504 |  1.3384244 | ⋯ |  0.8735983 |  0.7651778 | -1.7634525 |  0.4803649 |  0.31325164 | -0.9264642 | -0.8271999 |  2.6599423 | -1.0749029 |  0.44507673 |\n| 6 |  519.0936 | 10.91022 |  0.3820876 |  2.09306799 | -1.6259751 | -0.48786673 | -0.3655133 | -0.4582505 | -0.1920236 |  0.6668734 | ⋯ |  0.2263574 | -0.4766437 | -2.8152937 | -0.9024205 |  0.02830745 |  1.3253200 |  0.3081907 |  0.5242000 | -0.4868474 |  1.78749391 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.frame: 6 × 101</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>Y</th><th scope=col>X1</th><th scope=col>X2</th><th scope=col>X3</th><th scope=col>X4</th><th scope=col>X5</th><th scope=col>X6</th><th scope=col>X7</th><th scope=col>X8</th><th scope=col>X9</th><th scope=col>⋯</th><th scope=col>X91</th><th scope=col>X92</th><th scope=col>X93</th><th scope=col>X94</th><th scope=col>X95</th><th scope=col>X96</th><th scope=col>X97</th><th scope=col>X98</th><th scope=col>X99</th><th scope=col>X100</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>⋯</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>1</th><td> 798.3495</td><td>22.92764</td><td>-4.5586971</td><td> 2.12746303</td><td>-0.2915456</td><td>-0.78486098</td><td>-0.8257089</td><td> 0.2092830</td><td> 0.7548681</td><td>-0.2624089</td><td>⋯</td><td> 0.2170089</td><td> 0.9811519</td><td>-0.1497657</td><td> 2.7536749</td><td>-0.07124450</td><td>-0.3998977</td><td> 0.9764391</td><td>-0.3016068</td><td>-1.4666577</td><td>-0.28744876</td></tr>\n",
              "\t<tr><th scope=row>2</th><td> 765.7054</td><td>23.54733</td><td>-0.2452235</td><td>-0.18770648</td><td> 0.9726278</td><td>-2.56005244</td><td> 0.4902205</td><td> 0.5836894</td><td> 0.2102209</td><td>-1.2662017</td><td>⋯</td><td> 0.5958067</td><td> 1.3969812</td><td> 0.5189769</td><td> 0.8732472</td><td> 0.83001130</td><td>-1.3213831</td><td>-0.9214097</td><td>-1.2180848</td><td>-0.9668004</td><td> 0.03717712</td></tr>\n",
              "\t<tr><th scope=row>3</th><td> 776.4606</td><td>19.45348</td><td>-2.0269374</td><td> 0.09875784</td><td>-0.2397067</td><td> 0.07280078</td><td>-0.9265319</td><td>-0.6860822</td><td>-0.6535798</td><td>-0.9027523</td><td>⋯</td><td> 0.8350547</td><td>-0.2559876</td><td> 0.8028454</td><td>-0.5068545</td><td>-0.31125485</td><td>-0.4995762</td><td> 0.5872777</td><td> 0.4780813</td><td>-0.8942023</td><td>-1.34888323</td></tr>\n",
              "\t<tr><th scope=row>4</th><td> 813.3765</td><td>17.73251</td><td> 5.6519090</td><td> 1.91037815</td><td>-0.9017177</td><td> 0.75024358</td><td> 1.6025499</td><td>-1.0184904</td><td> 0.2518509</td><td> 0.1460915</td><td>⋯</td><td>-0.7519070</td><td> 1.2994639</td><td> 0.5629971</td><td> 1.4382039</td><td>-0.51459724</td><td>-0.6148111</td><td> 1.5750744</td><td> 0.0106562</td><td>-0.9325735</td><td>-0.76406130</td></tr>\n",
              "\t<tr><th scope=row>5</th><td>1014.5100</td><td>23.02944</td><td> 4.0773237</td><td> 1.62145572</td><td>-0.6311743</td><td>-0.12824888</td><td>-0.8608910</td><td>-1.1347534</td><td>-0.4156504</td><td> 1.3384244</td><td>⋯</td><td> 0.8735983</td><td> 0.7651778</td><td>-1.7634525</td><td> 0.4803649</td><td> 0.31325164</td><td>-0.9264642</td><td>-0.8271999</td><td> 2.6599423</td><td>-1.0749029</td><td> 0.44507673</td></tr>\n",
              "\t<tr><th scope=row>6</th><td> 519.0936</td><td>10.91022</td><td> 0.3820876</td><td> 2.09306799</td><td>-1.6259751</td><td>-0.48786673</td><td>-0.3655133</td><td>-0.4582505</td><td>-0.1920236</td><td> 0.6668734</td><td>⋯</td><td> 0.2263574</td><td>-0.4766437</td><td>-2.8152937</td><td>-0.9024205</td><td> 0.02830745</td><td> 1.3253200</td><td> 0.3081907</td><td> 0.5242000</td><td>-0.4868474</td><td> 1.78749391</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP5KdO10b4Qt"
      },
      "source": [
        "We defined the 'true' relation between Y and X1 and X2, and added additional variables X3 - X100, that have nothing to do with the outcome Y.\n",
        "\n",
        "Now that we have this data, we can explore to what extent machine learning is able to distinguish between the relevant predictors X1 and X2, and the irrelevant predictors X3 - X100."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_jyJNQQce-u"
      },
      "source": [
        "##### step 2: split the data into training and test set\n",
        "set.seed(1)\n",
        "train <- createDataPartition(df$Y,p = 0.7, list = FALSE)\n",
        "df_train <- df[train,]\n",
        "df_test <- df[-train,]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQBvV3Gvcj4I",
        "outputId": "217510f6-abad-4feb-c497-bff9e8dd758b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "##### step 3: train model using all data\n",
        "\n",
        "# define cross-validation strategy\n",
        "fitControl <- trainControl(## 10-fold CV\n",
        "                        method = \"repeatedcv\",\n",
        "                        number = 10,    ## repeated ten times\n",
        "                        repeats = 10)\n",
        "\n",
        "# train model (LASSO)\n",
        "lambda <- 10^seq(-3,3,length=100)\n",
        "set.seed(825)\n",
        "LassoFit <- train(Y ~ ., data = df_train, \n",
        "                 method = \"glmnet\", \n",
        "                 trControl = fitControl,\n",
        "                 tuneGrid = expand.grid(alpha = 1, lambda = lambda))\n",
        "LassoFit\n",
        "LassoFit$finalModel$tuneValue\n",
        "\n",
        "coef(LassoFit$finalModel, LassoFit$bestTune$lambda)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
            "“There were missing values in resampled performance measures.”\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "glmnet \n",
              "\n",
              "128 samples\n",
              "100 predictors\n",
              "\n",
              "No pre-processing\n",
              "Resampling: Cross-Validated (10 fold, repeated 10 times) \n",
              "Summary of sample sizes: 115, 116, 115, 116, 115, 115, ... \n",
              "Resampling results across tuning parameters:\n",
              "\n",
              "  lambda        RMSE       Rsquared   MAE      \n",
              "  1.000000e-03  261.24390  0.3790358  214.96968\n",
              "  1.149757e-03  261.24390  0.3790358  214.96968\n",
              "  1.321941e-03  261.24390  0.3790358  214.96968\n",
              "  1.519911e-03  261.24390  0.3790358  214.96968\n",
              "  1.747528e-03  261.24390  0.3790358  214.96968\n",
              "  2.009233e-03  261.24390  0.3790358  214.96968\n",
              "  2.310130e-03  261.24390  0.3790358  214.96968\n",
              "  2.656088e-03  261.24390  0.3790358  214.96968\n",
              "  3.053856e-03  261.24390  0.3790358  214.96968\n",
              "  3.511192e-03  261.24390  0.3790358  214.96968\n",
              "  4.037017e-03  261.24390  0.3790358  214.96968\n",
              "  4.641589e-03  261.24390  0.3790358  214.96968\n",
              "  5.336699e-03  261.24390  0.3790358  214.96968\n",
              "  6.135907e-03  261.24390  0.3790358  214.96968\n",
              "  7.054802e-03  261.24390  0.3790358  214.96968\n",
              "  8.111308e-03  261.24390  0.3790358  214.96968\n",
              "  9.326033e-03  261.24390  0.3790358  214.96968\n",
              "  1.072267e-02  261.24390  0.3790358  214.96968\n",
              "  1.232847e-02  261.24390  0.3790358  214.96968\n",
              "  1.417474e-02  261.24390  0.3790358  214.96968\n",
              "  1.629751e-02  261.24390  0.3790358  214.96968\n",
              "  1.873817e-02  261.24390  0.3790358  214.96968\n",
              "  2.154435e-02  261.11833  0.3791749  214.86591\n",
              "  2.477076e-02  260.58557  0.3798102  214.42927\n",
              "  2.848036e-02  259.77154  0.3809696  213.75752\n",
              "  3.274549e-02  258.67701  0.3826436  212.84340\n",
              "  3.764936e-02  257.27117  0.3848921  211.72089\n",
              "  4.328761e-02  255.50444  0.3877838  210.32179\n",
              "  4.977024e-02  253.44182  0.3912011  208.68137\n",
              "  5.722368e-02  251.00242  0.3953250  206.75494\n",
              "  6.579332e-02  248.23875  0.4001913  204.56725\n",
              "  7.564633e-02  245.24014  0.4054459  202.17684\n",
              "  8.697490e-02  241.89925  0.4115485  199.52188\n",
              "  1.000000e-01  238.17250  0.4187612  196.53714\n",
              "  1.149757e-01  234.11006  0.4269590  193.23721\n",
              "  1.321941e-01  229.69484  0.4361625  189.65058\n",
              "  1.519911e-01  224.97619  0.4463423  185.83836\n",
              "  1.747528e-01  219.92670  0.4577153  181.70387\n",
              "  2.009233e-01  214.50163  0.4702652  177.24595\n",
              "  2.310130e-01  208.97894  0.4835242  172.70409\n",
              "  2.656088e-01  203.30137  0.4976032  168.06859\n",
              "  3.053856e-01  197.53514  0.5124384  163.34192\n",
              "  3.511192e-01  191.80867  0.5280276  158.54481\n",
              "  4.037017e-01  185.92791  0.5448493  153.55345\n",
              "  4.641589e-01  179.98055  0.5624853  148.45472\n",
              "  5.336699e-01  174.06445  0.5805393  143.32635\n",
              "  6.135907e-01  168.37073  0.5979251  138.42626\n",
              "  7.054802e-01  162.88452  0.6150203  133.73130\n",
              "  8.111308e-01  157.66174  0.6317614  129.20198\n",
              "  9.326033e-01  152.65721  0.6479397  124.77240\n",
              "  1.072267e+00  147.78429  0.6638485  120.52673\n",
              "  1.232847e+00  143.18554  0.6788246  116.55524\n",
              "  1.417474e+00  138.75588  0.6933745  112.68339\n",
              "  1.629751e+00  134.55079  0.7072919  109.02031\n",
              "  1.873817e+00  130.48648  0.7209340  105.49860\n",
              "  2.154435e+00  126.76338  0.7336015  102.39513\n",
              "  2.477076e+00  123.34211  0.7454090   99.52004\n",
              "  2.848036e+00  120.06123  0.7568687   96.76557\n",
              "  3.274549e+00  116.95639  0.7680901   94.13886\n",
              "  3.764936e+00  114.22922  0.7783264   91.86173\n",
              "  4.328761e+00  111.81842  0.7877095   89.84429\n",
              "  4.977024e+00  109.48824  0.7969557   87.92761\n",
              "  5.722368e+00  107.15789  0.8060499   86.11088\n",
              "  6.579332e+00  104.97530  0.8144882   84.42227\n",
              "  7.564633e+00  103.00187  0.8221386   82.76846\n",
              "  8.697490e+00  101.32487  0.8288419   81.26212\n",
              "  1.000000e+01  100.28397  0.8335144   80.12176\n",
              "  1.149757e+01   99.84786  0.8363874   79.54049\n",
              "  1.321941e+01   99.66517  0.8385888   79.23405\n",
              "  1.519911e+01   99.42070  0.8414507   79.00768\n",
              "  1.747528e+01   99.19632  0.8446065   79.05227\n",
              "  2.009233e+01   99.68146  0.8460099   79.75213\n",
              "  2.310130e+01  101.12488  0.8447369   81.38628\n",
              "  2.656088e+01  103.24060  0.8420580   83.66694\n",
              "  3.053856e+01  105.99241  0.8382184   86.59006\n",
              "  3.511192e+01  109.57934  0.8323612   90.17666\n",
              "  4.037017e+01  114.13549  0.8237732   94.43812\n",
              "  4.641589e+01  119.85671  0.8110888   99.64719\n",
              "  5.336699e+01  126.92576  0.7922221  105.85502\n",
              "  6.135907e+01  134.60509  0.7685423  112.42245\n",
              "  7.054802e+01  140.17912  0.7597455  117.24130\n",
              "  8.111308e+01  145.68508  0.7596236  121.92491\n",
              "  9.326033e+01  152.65785  0.7596236  127.49919\n",
              "  1.072267e+02  161.42753  0.7596236  134.18508\n",
              "  1.232847e+02  172.35376  0.7596236  142.28553\n",
              "  1.417474e+02  185.83320  0.7596236  152.57390\n",
              "  1.629751e+02  202.29825  0.7596236  165.83471\n",
              "  1.873817e+02  222.21854  0.7596236  181.86740\n",
              "  2.154435e+02  238.54516        NaN  195.52235\n",
              "  2.477076e+02  238.54516        NaN  195.52235\n",
              "  2.848036e+02  238.54516        NaN  195.52235\n",
              "  3.274549e+02  238.54516        NaN  195.52235\n",
              "  3.764936e+02  238.54516        NaN  195.52235\n",
              "  4.328761e+02  238.54516        NaN  195.52235\n",
              "  4.977024e+02  238.54516        NaN  195.52235\n",
              "  5.722368e+02  238.54516        NaN  195.52235\n",
              "  6.579332e+02  238.54516        NaN  195.52235\n",
              "  7.564633e+02  238.54516        NaN  195.52235\n",
              "  8.697490e+02  238.54516        NaN  195.52235\n",
              "  1.000000e+03  238.54516        NaN  195.52235\n",
              "\n",
              "Tuning parameter 'alpha' was held constant at a value of 1\n",
              "RMSE was used to select the optimal model using the smallest value.\n",
              "The final values used for the model were alpha = 1 and lambda = 17.47528."
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   alpha lambda  \n",
              "71 1     17.47528"
            ],
            "text/latex": "A data.frame: 1 × 2\n\\begin{tabular}{r|ll}\n  & alpha & lambda\\\\\n  & <dbl> & <dbl>\\\\\n\\hline\n\t71 & 1 & 17.47528\\\\\n\\end{tabular}\n",
            "text/markdown": "\nA data.frame: 1 × 2\n\n| <!--/--> | alpha &lt;dbl&gt; | lambda &lt;dbl&gt; |\n|---|---|---|\n| 71 | 1 | 17.47528 |\n\n",
            "text/html": [
              "<table>\n",
              "<caption>A data.frame: 1 × 2</caption>\n",
              "<thead>\n",
              "\t<tr><th></th><th scope=col>alpha</th><th scope=col>lambda</th></tr>\n",
              "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><th scope=row>71</th><td>1</td><td>17.47528</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "101 x 1 sparse Matrix of class \"dgCMatrix\"\n",
              "                      1\n",
              "(Intercept) 94.10991091\n",
              "X1          36.38231342\n",
              "X2          12.50758018\n",
              "X3           .         \n",
              "X4           .         \n",
              "X5           .         \n",
              "X6           .         \n",
              "X7           .         \n",
              "X8           .         \n",
              "X9           .         \n",
              "X10          .         \n",
              "X11          .         \n",
              "X12          .         \n",
              "X13          .         \n",
              "X14          .         \n",
              "X15          .         \n",
              "X16          .         \n",
              "X17          .         \n",
              "X18          .         \n",
              "X19          .         \n",
              "X20          .         \n",
              "X21          .         \n",
              "X22          .         \n",
              "X23          .         \n",
              "X24          .         \n",
              "X25          .         \n",
              "X26          .         \n",
              "X27          .         \n",
              "X28          .         \n",
              "X29          .         \n",
              "X30          .         \n",
              "X31          .         \n",
              "X32          .         \n",
              "X33          .         \n",
              "X34          .         \n",
              "X35          .         \n",
              "X36          .         \n",
              "X37          .         \n",
              "X38          .         \n",
              "X39          .         \n",
              "X40          .         \n",
              "X41          .         \n",
              "X42          .         \n",
              "X43          .         \n",
              "X44          .         \n",
              "X45          .         \n",
              "X46          .         \n",
              "X47          .         \n",
              "X48          .         \n",
              "X49          7.71372918\n",
              "X50          .         \n",
              "X51          .         \n",
              "X52          .         \n",
              "X53          .         \n",
              "X54          .         \n",
              "X55          .         \n",
              "X56          .         \n",
              "X57          .         \n",
              "X58          .         \n",
              "X59          .         \n",
              "X60          .         \n",
              "X61          .         \n",
              "X62          .         \n",
              "X63          .         \n",
              "X64          .         \n",
              "X65          .         \n",
              "X66          .         \n",
              "X67          .         \n",
              "X68          .         \n",
              "X69          .         \n",
              "X70          .         \n",
              "X71          .         \n",
              "X72          .         \n",
              "X73          .         \n",
              "X74          .         \n",
              "X75          .         \n",
              "X76          .         \n",
              "X77          .         \n",
              "X78          .         \n",
              "X79          0.07333259\n",
              "X80          .         \n",
              "X81          .         \n",
              "X82          .         \n",
              "X83          .         \n",
              "X84          .         \n",
              "X85          0.14625811\n",
              "X86          .         \n",
              "X87          .         \n",
              "X88          .         \n",
              "X89         -2.08161809\n",
              "X90          .         \n",
              "X91          .         \n",
              "X92          .         \n",
              "X93          .         \n",
              "X94          .         \n",
              "X95          .         \n",
              "X96          .         \n",
              "X97          .         \n",
              "X98          .         \n",
              "X99          .         \n",
              "X100         .         "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dR400H5lfpUl"
      },
      "source": [
        "This LASSO-model, which is specifically developed to distinguish between relevant and irrelevant predictors, includes the relevant predictors X1 and X2, but also includes 4 other predictors (and manages to exclude 94 out of 98 irrelevant predictors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THMLYLM6fatK",
        "outputId": "65e3a937-34e2-4e97-ac45-d6a9f3d754f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##### step 4: evaluate performance of full model\n",
        "error_full <- df_test[,1] - predict(LassoFit, df_test)\n",
        "rmse_full <- sqrt(mean(error_full^2))\n",
        "mae_full <- mean(abs(error_full))\n",
        "print(paste('RMSE for model using all predictors:', rmse_full))\n",
        "print(paste('MAE for model using all predictors: ', mae_full))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] \"RMSE for model using all predictors: 110.207866557162\"\n",
            "[1] \"MAE for model using all predictors:  89.0088336830163\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2HJVy6fgbAh",
        "outputId": "c5267d5d-6f93-425a-bc96-69170ee395e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "##### step 5: train model using only the relevant data\n",
        "model_causal <- lm(Y~X1+X2, data = df_train)\n",
        "summary(model_causal)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\n",
              "Call:\n",
              "lm(formula = Y ~ X1 + X2, data = df_train)\n",
              "\n",
              "Residuals:\n",
              "     Min       1Q   Median       3Q      Max \n",
              "-244.825  -51.130    0.789   61.179  298.566 \n",
              "\n",
              "Coefficients:\n",
              "            Estimate Std. Error t value Pr(>|t|)    \n",
              "(Intercept)    9.104     32.650   0.279    0.781    \n",
              "X1            40.462      1.544  26.211  < 2e-16 ***\n",
              "X2            17.610      1.884   9.348 4.49e-16 ***\n",
              "---\n",
              "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
              "\n",
              "Residual standard error: 94.46 on 125 degrees of freedom\n",
              "Multiple R-squared:  0.8492,\tAdjusted R-squared:  0.8468 \n",
              "F-statistic:   352 on 2 and 125 DF,  p-value: < 2.2e-16\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlOhVE2jggXn",
        "outputId": "34bf3c87-add7-4415-ac57-56fb401001ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "##### step 6: compare the performance of both models\n",
        "error_causal <- df_test[,1] - predict(model_causal, df_test)\n",
        "rmse_causal <- sqrt(mean(error_causal^2))\n",
        "mae_causal <- mean(abs(error_causal))\n",
        "\n",
        "print(paste('RMSE for model using only relevant predictors:', rmse_causal))\n",
        "print(paste('MAE for model using only relevant predictors: ', mae_causal))\n",
        "\n",
        "print(paste('% improvement (RMSE) compared to using all predictors:', (rmse_full - rmse_causal) / rmse_full * 100, '%'))\n",
        "print(paste('% improvement (MAE) compared to using all predictors: ', (mae_full - mae_causal) / mae_full * 100, '%'))\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1] \"RMSE for model using only relevant predictors: 99.6668873023072\"\n",
            "[1] \"MAE for model using only relevant predictors:  80.4775732788778\"\n",
            "[1] \"% improvement (RMSE) compared to using all predictors: 9.5646341628321 %\"\n",
            "[1] \"% improvement (MAE) compared to using all predictors:  9.58473451581291 %\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvldnHauhLos"
      },
      "source": [
        "# Take away\n",
        "- Machine learning does not perfectly detect which predictors truly matter (though it does a fairly good job trying)\n",
        "- it is always important to appraoch prediction from a causal understanding of the problem at hand"
      ]
    }
  ]
}